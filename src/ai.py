import litellm
import re
import os
import json
import time
import ssl
from dotenv import load_dotenv
from prompts import get_personality_prompt

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ¨¡å‹æ˜ å°„
MODEL_CONFIG = {
    'openai': {
        'models': {
            'gpt-4': 'GPT-4',
            'gpt-4-turbo': 'GPT-4 Turbo',
            'gpt-3.5-turbo': 'GPT-3.5 Turbo',
            'gpt-4o': 'GPT-4o',
            'gpt-4o-mini': 'GPT-4o Mini'
        },
        'default_model': os.getenv('OPENAI_DEFAULT_MODEL', 'gpt-4'),
        'api_key_env': 'OPENAI_API_KEY',
        'api_base_env': 'OPENAI_API_BASE',
        'default_api_base': 'https://api.openai.com/v1'
    },
    'deepseek': {
        'models': {
            'deepseek/deepseek-chat': 'DeepSeek Chat',
            'deepseek/deepseek-coder': 'DeepSeek Coder'
        },
        'default_model': f"deepseek/{os.getenv('DEEPSEEK_DEFAULT_MODEL', 'deepseek-chat')}",
        'api_key_env': 'DEEPSEEK_API_KEY',
        'api_base_env': 'DEEPSEEK_API_BASE',
        'default_api_base': 'https://api.deepseek.com'
    },
    'openrouter': {
        'models': {
            'openrouter/anthropic/claude-3.5-sonnet': 'Claude 3.5 Sonnet',
            'openrouter/anthropic/claude-3-haiku': 'Claude 3 Haiku',
            'openrouter/openai/gpt-4': 'GPT-4 (OpenRouter)',
            'openrouter/meta-llama/llama-3.1-70b-instruct': 'Llama 3.1 70B',
            'openrouter/google/gemini-pro': 'Gemini Pro',
            'openrouter/google/gemini-2.5-flash': 'Gemini 2.5 Flash',
            'openrouter/qwen/qwen3-235b-a22b-thinking-2507': 'Qwen3 235B Thinking'
        },
        'default_model': f"openrouter/{os.getenv('OPENROUTER_DEFAULT_MODEL', 'google/gemini-2.5-flash')}",
        'api_key_env': 'OPENROUTER_API_KEY',
        'api_base_env': 'OPENROUTER_API_BASE',
        'default_api_base': 'https://openrouter.ai/api/v1'
    },
    'claude': {
        'models': {
            'claude-3-5-sonnet-20241022': 'Claude 3.5 Sonnet',
            'claude-3-sonnet-20240229': 'Claude 3 Sonnet',
            'claude-3-haiku-20240307': 'Claude 3 Haiku'
        },
        'default_model': os.getenv('CLAUDE_DEFAULT_MODEL', 'claude-3-5-sonnet-20241022'),
        'api_key_env': 'ANTHROPIC_API_KEY',
        'api_base_env': 'ANTHROPIC_API_BASE',
        'default_api_base': 'https://api.anthropic.com'
    },
    'custom': {
        'models': {
            'gpt-3.5-turbo': 'è‡ªå®šä¹‰æ¨¡å‹',
            'llama2': 'Llama 2',
            'mixtral-8x7b': 'Mixtral 8x7B'
        },
        'default_model': os.getenv('CUSTOM_DEFAULT_MODEL', os.getenv('CUSTOM_MODEL', 'gpt-3.5-turbo')),
        'api_key_env': 'CUSTOM_API_KEY',
        'api_base_env': 'CUSTOM_API_BASE',
        'default_api_base': 'https://api.openai.com/v1'
    }
}

def get_model_config(provider: str = None):
    """è·å–æ¨¡å‹é…ç½®"""
    if not provider:
        # é»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„æä¾›å•†
        for p, config in MODEL_CONFIG.items():
            if os.getenv(config['api_key_env']):
                provider = p
                break
        else:
            provider = 'openai'  # é»˜è®¤å›é€€åˆ°OpenAI
    
    return MODEL_CONFIG.get(provider, MODEL_CONFIG['openai'])


def setup_litellm(provider: str = None, model: str = None, api_key: str = None):
    """è®¾ç½®LiteLLMé…ç½®"""
    config = get_model_config(provider)
    
    # è®¾ç½®APIå¯†é’¥
    if api_key:
        os.environ[config['api_key_env']] = api_key
    
    # è®¾ç½®APIåŸºç¡€URL
    api_base = os.getenv(config['api_base_env'], config['default_api_base'])
    
    if provider == 'deepseek':
        litellm.api_base = api_base
        os.environ['DEEPSEEK_API_BASE'] = api_base
    elif provider == 'openrouter':
        os.environ['OPENROUTER_API_BASE'] = api_base
    elif provider == 'claude':
        os.environ['ANTHROPIC_API_BASE'] = api_base
    elif provider == 'custom':
        litellm.api_base = api_base
        os.environ['LITELLM_API_BASE'] = api_base
    else:  # openai
        if api_base != 'https://api.openai.com/v1':
            litellm.api_base = api_base
            os.environ['OPENAI_API_BASE'] = api_base
    
    # å¤„ç†æ¨¡å‹åç§°
    final_model = model or config['default_model']
    
    # å¤„ç†ä¸åŒæä¾›å•†çš„æ¨¡å‹åç§°æ ¼å¼
    if provider == 'openrouter':
        # OpenRouteréœ€è¦openrouter/å‰ç¼€æ‰èƒ½è¢«litellmè¯†åˆ«
        if not final_model.startswith('openrouter/'):
            final_model = f'openrouter/{final_model}'
    elif provider == 'deepseek':
        # DeepSeekéœ€è¦ç‰¹å®šå‰ç¼€
        if final_model and '/' not in final_model:
            final_model = f'deepseek/{final_model}'
    elif provider == 'claude':
        # Claudeæ¨¡å‹é€šå¸¸ä¸éœ€è¦å‰ç¼€
        pass
    elif provider == 'custom':
        # è‡ªå®šä¹‰æä¾›å•†ï¼Œä¿æŒåŸæ ·
        pass
    # OpenAIæ¨¡å‹é€šå¸¸ä¸éœ€è¦å‰ç¼€
    
    return final_model

def is_yes_or_no_question(question: str, key: str, provider: str = None, model: str = None):
    """åˆ¤æ–­æ˜¯å¦ä¸ºæ˜¯/å¦é—®é¢˜ - å¸¦é‡è¯•æœºåˆ¶"""
    from prompts import YES_NO_QUESTION_PROMPT
    
    final_model = setup_litellm(provider, model, key)
    
    # é‡è¯•é…ç½®
    max_retries = 3
    retry_delay = 1  # ç§’
    
    for attempt in range(max_retries):
        try:
            # å°è¯•ä½¿ç”¨logit_biasï¼ˆä»…å¯¹OpenAIæ¨¡å‹æœ‰æ•ˆï¼‰
            extra_params = {}
            if provider == 'openai' or final_model.startswith('gpt'):
                extra_params['logit_bias'] = {
                    9642: 100,  # Yes
                    2822: 100   # No
                }
            
            # ä¸ºSSLé—®é¢˜æ·»åŠ ç‰¹æ®Šé…ç½®
            if provider == 'openrouter':
                extra_params.update({
                    'timeout': 30,
                    'max_retries': 2
                })
            
            # æ„å»ºè¯·æ±‚æ¶ˆæ¯
            messages = [
                {'role': 'system', 'content': YES_NO_QUESTION_PROMPT},
                {'role': 'user', 'content': question},
            ]
            
            # æ‰“å°åŸå§‹è¯·æ±‚
            print(f"\nğŸ” [DEBUG] æ˜¯éé¢˜åˆ¤æ–­ - å°è¯• {attempt + 1}/{max_retries}:")
            print(f"æ¨¡å‹: {final_model}")
            print(f"æä¾›å•†: {provider}")
            print(f"ç”¨æˆ·é—®é¢˜: {question}")
            
            response = litellm.completion(
                model=final_model,
                messages=messages,
                max_tokens=1,
                temperature=0,
                **extra_params
            )
            
            # æ‰“å°åŸå§‹å“åº”
            print(f"\nğŸ“¥ [DEBUG] æ˜¯éé¢˜åˆ¤æ–­ - æˆåŠŸå“åº”:")
            print(f"å“åº”å†…å®¹: {response.choices[0].message.content}")
            if hasattr(response, 'usage'):
                print(f"Tokenä½¿ç”¨: {response.usage}")

            content = response.choices[0].message.content.strip()

            if content == 'Yes':
                print(f"âœ… åˆ¤æ–­ç»“æœ: æ˜¯éé¢˜")
                return True
            elif content == 'No':
                print(f"âœ… åˆ¤æ–­ç»“æœ: å¼€æ”¾æ€§é—®é¢˜")
                return False
            else:
                # å¦‚æœä¸æ˜¯æ ‡å‡†çš„Yes/Noå›ç­”ï¼Œé»˜è®¤ä¸ºå¼€æ”¾æ€§é—®é¢˜
                print(f'âš ï¸ æ— æ•ˆçš„é—®é¢˜æ³¨é‡Šå“åº”: {content}, é»˜è®¤ä¸ºå¼€æ”¾æ€§é—®é¢˜')
                return False

        except Exception as e:
            error_msg = str(e)
            print(f"\nâŒ [DEBUG] æ˜¯éé¢˜åˆ¤æ–­ - å°è¯• {attempt + 1} å¤±è´¥:")
            print(f"é”™è¯¯ä¿¡æ¯: {error_msg}")
            print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯SSL/ç½‘ç»œé”™è¯¯
            is_network_error = any(keyword in error_msg.lower() for keyword in [
                'ssl', 'eof', 'connection', 'timeout', 'network', 'unexpected_eof'
            ])
            
            # å¦‚æœæ˜¯ç½‘ç»œé”™è¯¯ä¸”è¿˜æœ‰é‡è¯•æœºä¼šï¼Œåˆ™é‡è¯•
            if is_network_error and attempt < max_retries - 1:
                print(f"ğŸ”„ æ£€æµ‹åˆ°ç½‘ç»œé”™è¯¯ï¼Œ{retry_delay}ç§’åé‡è¯•...")
                time.sleep(retry_delay)
                retry_delay *= 2  # æŒ‡æ•°é€€é¿
                continue
            
            # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
            print(f"âš ï¸ æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œé»˜è®¤ä¸ºå¼€æ”¾æ€§é—®é¢˜")
            return False
    
    # ç†è®ºä¸Šä¸ä¼šåˆ°è¾¾è¿™é‡Œï¼Œä½†ä¸ºäº†å®‰å…¨èµ·è§
    return False

def get_system_prompt(personality: str):
    """è·å–ç³»ç»Ÿæç¤ºè¯ - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„äººæ ¼æç¤ºè¯"""
    return get_personality_prompt(personality)

def get_structured_answer(question: str, personality: str, is_yes_or_no: bool, key: str, provider: str = None, model: str = None):
    """æ ¹æ®é—®é¢˜ç±»å‹è·å–ç»“æ„åŒ–æˆ–è‡ªç„¶è¯­è¨€å›ç­” - å¸¦é‡è¯•æœºåˆ¶"""
    final_model = setup_litellm(provider, model, key)
    
    # é‡è¯•é…ç½®
    max_retries = 3
    retry_delay = 2  # ç§’
    
    for attempt in range(max_retries):
        try:
            # æ„å»ºç³»ç»Ÿæ¶ˆæ¯ï¼Œæ˜ç¡®å‘ŠçŸ¥é—®é¢˜ç±»å‹
            base_prompt = get_system_prompt(personality)
            if is_yes_or_no:
                system_message = f"{base_prompt}\n\né‡è¦æç¤ºï¼šè¿™æ˜¯ä¸€ä¸ªæ˜¯éé¢˜ï¼Œè¯·æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºã€‚"
            else:
                system_message = f"{base_prompt}\n\né‡è¦æç¤ºï¼šè¿™æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§é—®é¢˜ï¼Œè¯·ç›´æ¥è¾“å‡ºè‡ªç„¶è¯­è¨€å›ç­”ã€‚"
            
            # æ„å»ºè¯·æ±‚æ¶ˆæ¯
            messages = [
                {'role': 'system', 'content': system_message},
                {'role': 'user', 'content': question},
            ]
            
            # æ‰“å°åŸå§‹è¯·æ±‚
            print(f"\nğŸ¤– [DEBUG] ç»“æ„åŒ–å›ç­” - å°è¯• {attempt + 1}/{max_retries}:")
            print(f"æ¨¡å‹: {final_model}")
            print(f"æä¾›å•†: {provider}")
            print(f"é—®é¢˜ç±»å‹: {'æ˜¯éé¢˜' if is_yes_or_no else 'å¼€æ”¾æ€§é—®é¢˜'}")
            print(f"æ¸©åº¦: 0.7")
            
            # ä¸ºSSLé—®é¢˜æ·»åŠ ç‰¹æ®Šé…ç½®
            extra_params = {}
            if provider == 'openrouter':
                # ä¸ºOpenRouteræ·»åŠ è¶…æ—¶å’Œé‡è¯•é…ç½®
                extra_params.update({
                    'timeout': 30,
                    'max_retries': 2
                })
            
            response = litellm.completion(
                model=final_model,
                messages=messages,
                temperature=0.7,
                **extra_params
            )
            
            # æ‰“å°åŸå§‹å“åº”
            print(f"\nğŸ“¥ [DEBUG] ç»“æ„åŒ–å›ç­” - æˆåŠŸå“åº”:")
            print(f"å“åº”å†…å®¹: {response.choices[0].message.content[:200]}{'...' if len(response.choices[0].message.content) > 200 else ''}")
            if hasattr(response, 'usage'):
                print(f"Tokenä½¿ç”¨: {response.usage}")

            return response.choices[0].message.content

        except Exception as e:
            error_msg = str(e)
            print(f"\nâŒ [DEBUG] ç»“æ„åŒ–å›ç­” - å°è¯• {attempt + 1} å¤±è´¥:")
            print(f"é”™è¯¯ä¿¡æ¯: {error_msg}")
            print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯SSL/ç½‘ç»œé”™è¯¯
            is_network_error = any(keyword in error_msg.lower() for keyword in [
                'ssl', 'eof', 'connection', 'timeout', 'network', 'unexpected_eof'
            ])
            
            # å¦‚æœæ˜¯ç½‘ç»œé”™è¯¯ä¸”è¿˜æœ‰é‡è¯•æœºä¼šï¼Œåˆ™é‡è¯•
            if is_network_error and attempt < max_retries - 1:
                print(f"ğŸ”„ æ£€æµ‹åˆ°ç½‘ç»œé”™è¯¯ï¼Œ{retry_delay}ç§’åé‡è¯•...")
                time.sleep(retry_delay)
                retry_delay *= 2  # æŒ‡æ•°é€€é¿
                continue
            
            # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥æˆ–éç½‘ç»œé”™è¯¯ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
            if "LLM Provider NOT provided" in error_msg:
                return f"æ¨¡å‹é…ç½®é”™è¯¯ï¼šæ— æ³•è¯†åˆ«æ¨¡å‹ '{final_model}'ã€‚\n\nè¯·æ£€æŸ¥æ¨¡å‹åç§°æ ¼å¼ï¼š\n- OpenAI: gpt-4, gpt-3.5-turbo\n- DeepSeek: deepseek-chat, deepseek-coder\n- OpenRouter: anthropic/claude-3.5-sonnet\n- è‡ªå®šä¹‰: è¯·ç¡®ä¿æ¨¡å‹åç§°æ­£ç¡®ä¸”APIå…¼å®¹"
            elif "api_key" in error_msg.lower():
                return f"APIå¯†é’¥é”™è¯¯ï¼šè¯·æ£€æŸ¥ {provider} çš„APIå¯†é’¥æ˜¯å¦æ­£ç¡®"
            elif "rate limit" in error_msg.lower():
                return f"è¯·æ±‚é¢‘ç‡é™åˆ¶ï¼šè¯·ç¨åå†è¯•"
            elif is_network_error:
                return f"ç½‘ç»œè¿æ¥é”™è¯¯ï¼šSSLè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•ã€‚\n\nå»ºè®®ï¼š\n1. æ£€æŸ¥ç½‘ç»œè¿æ¥\n2. å°è¯•åˆ‡æ¢åˆ°å…¶ä»–AIæä¾›å•†\n3. æ£€æŸ¥é˜²ç«å¢™è®¾ç½®"
            else:
                return f"è·å–å›ç­”æ—¶å‡ºé”™: {error_msg}"
    
    # ç†è®ºä¸Šä¸ä¼šåˆ°è¾¾è¿™é‡Œï¼Œä½†ä¸ºäº†å®‰å…¨èµ·è§
    return "æ‰€æœ‰é‡è¯•å°è¯•éƒ½å¤±è´¥äº†ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•ã€‚"


def fix_json(broken_json):
    """å°è¯•ä¿®å¤æŸåçš„JSONå­—ç¬¦ä¸²"""
    # 1. å°è¯•æå–answerå­—æ®µ
    answer_match = re.search(r'"answer"\s*:\s*"(.*?)(?:"|$)', broken_json, re.DOTALL)
    if answer_match:
        answer = answer_match.group(1).strip()
        # å¦‚æœå†…å®¹è¢«æˆªæ–­ï¼Œå°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„å¥å­
        if answer.endswith('...'):
            answer = re.sub(r'\.{3,}$', '', answer)  # ç§»é™¤ç»“å°¾çš„çœç•¥å·
        
        # 2. å°è¯•æå–åˆ†ç±»çŠ¶æ€
        status_match = re.search(r'"status"\s*:\s*"(\w+)"', broken_json)
        status = status_match.group(1) if status_match else "info"
        
        print(f"ğŸ”§ JSONä¿®å¤ - æå–å­—æ®µæˆåŠŸ")
        print(f"   æå–çš„å›ç­”: {answer[:50]}{'...' if len(answer) > 50 else ''}")
        print(f"   æå–çš„çŠ¶æ€: {status}")
        
        return answer, {'status': status, 'conditions': None}
    
    # 3. å¦‚æœæ²¡æœ‰æ˜ç¡®çš„JSONç»“æ„ï¼Œå°è¯•æå–æœ‰æ„ä¹‰çš„æ–‡æœ¬å†…å®¹
    # ç§»é™¤æ‰€æœ‰JSONæ ‡è®°å’Œå¼•å·
    text_only = re.sub(r'[{}\[\]":]', ' ', broken_json)
    text_only = re.sub(r'\s+', ' ', text_only).strip()
    
    # å¦‚æœæ–‡æœ¬ä»¥"answer"å¼€å¤´ï¼Œå°è¯•æå–åé¢çš„å†…å®¹
    if "answer" in text_only:
        parts = text_only.split("answer", 1)
        if len(parts) > 1:
            cleaned_text = parts[1].strip()
            if cleaned_text:
                return cleaned_text, {'status': 'info', 'conditions': None}
    
    # 4. å¦‚æœä»¥ä¸Šæ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›æ¸…ç†åçš„åŸå§‹å†…å®¹
    return broken_json, {'status': 'info', 'conditions': None}

def parse_structured_response(response_content: str, is_yes_or_no: bool):
    """è§£æç»“æ„åŒ–å“åº”"""
    if not is_yes_or_no:
        # å¼€æ”¾æ€§é—®é¢˜ç›´æ¥è¿”å›æ–‡æœ¬
        return response_content, {'status': 'info', 'conditions': None}
    
    # æ¸…ç†å“åº”å†…å®¹ï¼Œç§»é™¤markdownä»£ç å—æ ‡è®°
    cleaned_content = response_content.strip()
    
    # ç§»é™¤markdownä»£ç å—æ ‡è®°
    if cleaned_content.startswith('```json'):
        cleaned_content = cleaned_content[7:]  # ç§»é™¤ ```json
    elif cleaned_content.startswith('```'):
        cleaned_content = cleaned_content[3:]   # ç§»é™¤ ```
    
    if cleaned_content.endswith('```'):
        cleaned_content = cleaned_content[:-3]  # ç§»é™¤ç»“å°¾çš„ ```
    
    cleaned_content = cleaned_content.strip()
    
    try:
        # å°è¯•è§£æJSON
        result = json.loads(cleaned_content)
        answer = result.get("answer", response_content)
        classification = result.get("classification", {'status': 'info', 'conditions': None})
        
        print(f"âœ… JSONè§£ææˆåŠŸ")
        print(f"   å›ç­”: {answer[:50]}{'...' if len(str(answer)) > 50 else ''}")
        print(f"   åˆ†ç±»çŠ¶æ€: {classification.get('status', 'unknown')}")
        
        return answer, classification
    except (json.JSONDecodeError, KeyError) as e:
        print(f"âŒ JSONè§£æå¤±è´¥: {e}")
        print(f"   åŸå§‹å†…å®¹: {response_content[:100]}{'...' if len(response_content) > 100 else ''}")
        print(f"   æ¸…ç†åå†…å®¹: {cleaned_content[:100]}{'...' if len(cleaned_content) > 100 else ''}")
        
        # å°è¯•ä¿®å¤æŸåçš„JSON
        try:
            answer, classification = fix_json(cleaned_content)
            return answer, classification
        except Exception as repair_error:
            print(f"âŒ JSONä¿®å¤å¤±è´¥: {repair_error}")
            # å¦‚æœJSONè§£æå’Œä¿®å¤éƒ½å¤±è´¥ï¼Œè¿”å›åŸå§‹å†…å®¹å’Œé»˜è®¤åˆ†ç±»
            return response_content, {'status': 'info', 'conditions': None}

